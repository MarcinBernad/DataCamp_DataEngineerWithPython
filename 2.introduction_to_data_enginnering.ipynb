{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From task to subtask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parralel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of partitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions=4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel computation frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master local[4] /home/repl/spark-script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow scheduling framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow DAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()[\"score\"]\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from a databate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract table to a pandas DataFrame\n",
    "def extract_table_to_pandas(tablename, db_engine):\n",
    "    query = \"SELECT * FROM {}\".format(tablename)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "# Connect to the database using the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Extract the film table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"film\", db_engine)\n",
    "\n",
    "# Extract the customer table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"customer\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the rental rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rental rate column as a string\n",
    "rental_rate_str = film_df.rental_rate.astype(\"str\")\n",
    "\n",
    "# Split up and expand the column\n",
    "rental_rate_expanded = rental_rate_str.str.split(\".\", expand=True)\n",
    "\n",
    "# Assign the columns to film_df\n",
    "film_df = film_df.assign(\n",
    "    rental_rate_dollar=rental_rate_expanded[0],\n",
    "    rental_rate_cents=rental_rate_expanded[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/pagila\",\n",
    "                \"customer\",\n",
    "                {\"user\":\"repl\",\"password\":\"password\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas vs PySpark:\n",
    "\n",
    "df.info() == df.printSchema()\n",
    "\n",
    "df.head(2) == df.show(2) == df.limit(2).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupBy and mean to aggregate the column\n",
    "ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')\n",
    "\n",
    "# Join the tables using the film_id column\n",
    "film_df_with_ratings = film_df.join(\n",
    "    ratings_per_film_df,\n",
    "    film_df.film_id==ratings_per_film_df.film_id\n",
    ")\n",
    "\n",
    "# Show the 5 first results\n",
    "print(film_df_with_ratings.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the pandas DataFrame to parquet\n",
    "film_pdf.to_parquet(\"films_pdf.parquet\")\n",
    "\n",
    "# Write the PySpark DataFrame to parquet\n",
    "film_sdf.write.parquet(\"films_sdf.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine_dwh = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Transformation step, join with recommendations data\n",
    "film_pdf_joined = film_pdf.join(recommendations)\n",
    "\n",
    "# Finish the .to_sql() call to write to store.film\n",
    "film_pdf_joined.to_sql(\n",
    "    \"film\", \n",
    "    db_engine_dwh, \n",
    "    schema=\"store\", \n",
    "    if_exists=\"replace\")\n",
    "\n",
    "# Run the query to fetch the data\n",
    "pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", db_engine_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ETL function\n",
    "def etl():\n",
    "    film_df = extract_film_to_pandas()\n",
    "\n",
    "    film_df = transform_rental_rate(film_df)\n",
    "\n",
    "    load_dataframe_to_film(film_df)\n",
    "\n",
    "# Define the ETL task using PythonOperator\n",
    "etl_task = PythonOperator(task_id='etl_film',\n",
    "                          python_callable=etl,\n",
    "                          dag=dag)\n",
    "\n",
    "# Set the upstream to wait_for_table and sample run etl()\n",
    "etl_task.set_upstream(wait_for_table)\n",
    "etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ETL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_to_df(tablename, db_engine):\n",
    "    return pd.read_sql(\"Select * from {}\".format(tablename), db_engine)\n",
    "\n",
    "def split_columns_transform(df, column, pat, suffixes):\n",
    "    #....\n",
    "\n",
    "def load_df_into_dwh(film_df, tablename, schema, db_engine):\n",
    "    return pd.to_sql(tablename, db_engine, schema=schema, if_exist=\"replace\")\n",
    "\n",
    "\n",
    "db_engines = {...} # needs to be configured\n",
    "\n",
    "def etl():\n",
    "    # extract\n",
    "    film_df = extract_table_to_df(\"film\", db_engines['store'])\n",
    "\n",
    "    # transform\n",
    "    film_df = split_columns_transform(film_df, \"rental_rate\", \".\", ['_dollars','_cents'])\n",
    "\n",
    "    # load\n",
    "    load_df_into_dwh(film_df, \"film\", \"store\", db_engines['dwh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling with DAGs in Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "dag = DAG(dag_id = 'etl_pipeline',\n",
    "    schedule_interval = \"0 0 * * *\")\n",
    "\n",
    "etl_task = PythonOperator(task_id='etl_task',\n",
    "                python_callable=etl,\n",
    "                dag=dag)\n",
    "\n",
    "etl_task.set_upstream(wait_for_this_task)\n",
    "\n",
    "\n",
    "## after that - zapisać jako plik .py i przenieść do folderu z dag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('spyder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c264c83364d5fddb30ee8587131bfa53e93849ca537b0d68eb8252423e4ab77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
