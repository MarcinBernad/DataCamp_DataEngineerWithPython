{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From task to subtask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parralel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of partitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions=4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel computation frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master local[4] /home/repl/spark-script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow scheduling framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow DAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()[\"score\"]\n",
    "print(post_score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
